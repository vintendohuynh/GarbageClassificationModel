{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vintendohuynh/GarbageClassificationModel/blob/main/GarbageMLModel_CS549.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMPskyL7lbyA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import kagglehub\n",
        "from huggingface_hub import login, upload_folder\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(source_dir, train_dir, val_dir, split_ratio=0.8):\n",
        "    if not os.path.exists(train_dir):\n",
        "        os.makedirs(train_dir)\n",
        "    if not os.path.exists(val_dir):\n",
        "        os.makedirs(val_dir)\n",
        "\n",
        "    for class_name in os.listdir(source_dir):\n",
        "        class_dir = os.path.join(source_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            train_class_dir = os.path.join(train_dir, class_name)\n",
        "            val_class_dir = os.path.join(val_dir, class_name)\n",
        "            if not os.path.exists(train_class_dir):\n",
        "                os.makedirs(train_class_dir)\n",
        "            if not os.path.exists(val_class_dir):\n",
        "                os.makedirs(val_class_dir)\n",
        "\n",
        "            files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "            random.shuffle(files)\n",
        "            split_index = int(len(files) * split_ratio)\n",
        "            train_files = files[:split_index]\n",
        "            val_files = files[split_index:]\n",
        "\n",
        "            print(f\"Class: {class_name}, Total files: {len(files)}, Train files: {len(train_files)}, Val files: {len(val_files)}\")\n",
        "\n",
        "            for file in train_files:\n",
        "                shutil.copy(os.path.join(class_dir, file), os.path.join(train_class_dir, file))\n",
        "            for file in val_files:\n",
        "                shutil.copy(os.path.join(class_dir, file), os.path.join(val_class_dir, file))\n",
        "\n",
        "            print(f\"Copied {len(train_files)} files to {train_class_dir}\")\n",
        "            print(f\"Copied {len(val_files)} files to {val_class_dir}\")"
      ],
      "metadata": {
        "id": "3sd-yX3xCYXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"asdasdasasdas/garbage-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnJ0-miiCYpC",
        "outputId": "fda5d178-ae8a-4292-aad7-764c8d9846e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/asdasdasasdas/garbage-classification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.0M/82.0M [00:03<00:00, 25.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = os.path.join(path, 'Garbage classification', 'Garbage classification')\n",
        "train_dir = os.path.join(path, 'train')\n",
        "val_dir = os.path.join(path, 'val')\n",
        "\n",
        "# Split the dataset\n",
        "split_dataset(source_dir, train_dir, val_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4_KyQPECqKF",
        "outputId": "9f455501-c43d-478d-fba8-20caf8a9ae80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: metal, Total files: 410, Train files: 328, Val files: 82\n",
            "Copied 328 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/metal\n",
            "Copied 82 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/metal\n",
            "Class: plastic, Total files: 482, Train files: 385, Val files: 97\n",
            "Copied 385 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/plastic\n",
            "Copied 97 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/plastic\n",
            "Class: glass, Total files: 501, Train files: 400, Val files: 101\n",
            "Copied 400 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/glass\n",
            "Copied 101 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/glass\n",
            "Class: paper, Total files: 594, Train files: 475, Val files: 119\n",
            "Copied 475 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/paper\n",
            "Copied 119 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/paper\n",
            "Class: cardboard, Total files: 403, Train files: 322, Val files: 81\n",
            "Copied 322 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/cardboard\n",
            "Copied 81 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/cardboard\n",
            "Class: trash, Total files: 137, Train files: 109, Val files: 28\n",
            "Copied 109 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/train/trash\n",
            "Copied 28 files to /root/.cache/kagglehub/datasets/asdasdasasdas/garbage-classification/versions/2/val/trash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ],
      "metadata": {
        "id": "q-XiOjiVCv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "qbB8X3pECzaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Resize images to 150x150\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Multiple classes\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdVKdgeoC-zL",
        "outputId": "0deb327b-aa0d-4265-e52b-fe046e2961cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2019 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5WGMzqgDAuV",
        "outputId": "443a3157-42a3-45c0-f729-0c2f21ec7e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 508 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = models.Sequential()\n",
        "\n",
        "# First Convolutional Layer\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Second Convolutional Layer\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Third Convolutional Layer\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten the results to feed into a fully connected layer\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer for classification\n",
        "model.add(layers.Dense(train_generator.num_classes, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-09k3G64DD7y",
        "outputId": "724afc45-c514-4cfc-f96f-d3174567770b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',  # For multi-class classification\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "LURAXYGODHZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // 32,\n",
        "    epochs=50,  # You can adjust the number of epochs\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // 32\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(val_generator, steps=val_generator.samples // 32)\n",
        "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4W5tDanDJoh",
        "outputId": "32290bde-802d-474e-920a-afd30cd31cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2s/step - accuracy: 0.2049 - loss: 2.0086 - val_accuracy: 0.3979 - val_loss: 1.4735\n",
            "Epoch 2/50\n",
            "\u001b[1m 1/63\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 1s/step - accuracy: 0.3438 - loss: 1.5789"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3438 - loss: 1.5789 - val_accuracy: 0.2857 - val_loss: 1.6267\n",
            "Epoch 3/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.3891 - loss: 1.5017 - val_accuracy: 0.4313 - val_loss: 1.4538\n",
            "Epoch 4/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.4062 - loss: 1.4017 - val_accuracy: 0.5000 - val_loss: 1.3607\n",
            "Epoch 5/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.4513 - loss: 1.3621 - val_accuracy: 0.4875 - val_loss: 1.2543\n",
            "Epoch 6/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5625 - loss: 1.4533 - val_accuracy: 0.3929 - val_loss: 1.3175\n",
            "Epoch 7/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - accuracy: 0.5101 - loss: 1.2221 - val_accuracy: 0.5896 - val_loss: 1.1301\n",
            "Epoch 8/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5625 - loss: 1.0864 - val_accuracy: 0.5357 - val_loss: 1.1252\n",
            "Epoch 9/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.5777 - loss: 1.0983 - val_accuracy: 0.6021 - val_loss: 1.0746\n",
            "Epoch 10/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5312 - loss: 1.2506 - val_accuracy: 0.5000 - val_loss: 1.2303\n",
            "Epoch 11/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.5749 - loss: 1.0993 - val_accuracy: 0.5104 - val_loss: 1.2253\n",
            "Epoch 12/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4375 - loss: 1.4967 - val_accuracy: 0.6429 - val_loss: 0.8627\n",
            "Epoch 13/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.6001 - loss: 1.0552 - val_accuracy: 0.5479 - val_loss: 1.1924\n",
            "Epoch 14/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6875 - loss: 0.7824 - val_accuracy: 0.6071 - val_loss: 1.2109\n",
            "Epoch 15/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.6345 - loss: 0.9789 - val_accuracy: 0.5625 - val_loss: 1.0825\n",
            "Epoch 16/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4688 - loss: 1.1402 - val_accuracy: 0.6429 - val_loss: 1.3879\n",
            "Epoch 17/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 2s/step - accuracy: 0.6599 - loss: 0.8729 - val_accuracy: 0.6500 - val_loss: 1.0109\n",
            "Epoch 18/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5938 - loss: 0.9345 - val_accuracy: 0.6429 - val_loss: 0.9942\n",
            "Epoch 19/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2s/step - accuracy: 0.6879 - loss: 0.8454 - val_accuracy: 0.6396 - val_loss: 1.0236\n",
            "Epoch 20/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6875 - loss: 0.8361 - val_accuracy: 0.7143 - val_loss: 0.7895\n",
            "Epoch 21/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 2s/step - accuracy: 0.7070 - loss: 0.7781 - val_accuracy: 0.6271 - val_loss: 1.0229\n",
            "Epoch 22/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6562 - loss: 0.8064 - val_accuracy: 0.6429 - val_loss: 0.9865\n",
            "Epoch 23/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - accuracy: 0.7398 - loss: 0.7193 - val_accuracy: 0.6625 - val_loss: 0.9815\n",
            "Epoch 24/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7188 - loss: 0.8198 - val_accuracy: 0.5714 - val_loss: 1.0970\n",
            "Epoch 25/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.7230 - loss: 0.7225 - val_accuracy: 0.6292 - val_loss: 1.1361\n",
            "Epoch 26/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.6250 - loss: 1.0359 - val_accuracy: 0.6071 - val_loss: 1.3021\n",
            "Epoch 27/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 2s/step - accuracy: 0.7245 - loss: 0.7303 - val_accuracy: 0.6583 - val_loss: 1.0074\n",
            "Epoch 28/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7812 - loss: 0.4777 - val_accuracy: 0.5000 - val_loss: 1.2576\n",
            "Epoch 29/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.7868 - loss: 0.6044 - val_accuracy: 0.6438 - val_loss: 0.9235\n",
            "Epoch 30/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7812 - loss: 0.6780 - val_accuracy: 0.7143 - val_loss: 1.3482\n",
            "Epoch 31/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.8066 - loss: 0.5572 - val_accuracy: 0.6646 - val_loss: 1.0437\n",
            "Epoch 32/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7188 - loss: 0.6600 - val_accuracy: 0.7500 - val_loss: 0.7847\n",
            "Epoch 33/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.8231 - loss: 0.4908 - val_accuracy: 0.6875 - val_loss: 0.9713\n",
            "Epoch 34/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7812 - loss: 0.6010 - val_accuracy: 0.6786 - val_loss: 0.9410\n",
            "Epoch 35/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8444 - loss: 0.4791 - val_accuracy: 0.6583 - val_loss: 1.0471\n",
            "Epoch 36/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8750 - loss: 0.4512 - val_accuracy: 0.5000 - val_loss: 1.0121\n",
            "Epoch 37/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.8273 - loss: 0.5241 - val_accuracy: 0.6979 - val_loss: 0.9449\n",
            "Epoch 38/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.7188 - loss: 0.7566 - val_accuracy: 0.6786 - val_loss: 1.4550\n",
            "Epoch 39/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.8396 - loss: 0.4334 - val_accuracy: 0.6979 - val_loss: 1.0599\n",
            "Epoch 40/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.8438 - loss: 0.5250 - val_accuracy: 0.6786 - val_loss: 1.3375\n",
            "Epoch 41/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.8573 - loss: 0.4021 - val_accuracy: 0.7063 - val_loss: 1.0787\n",
            "Epoch 42/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8438 - loss: 0.5250 - val_accuracy: 0.5714 - val_loss: 1.3268\n",
            "Epoch 43/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.8930 - loss: 0.3436 - val_accuracy: 0.6687 - val_loss: 1.1329\n",
            "Epoch 44/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.3083 - val_accuracy: 0.8214 - val_loss: 0.6297\n",
            "Epoch 45/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.8602 - loss: 0.3988 - val_accuracy: 0.7271 - val_loss: 0.9835\n",
            "Epoch 46/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8438 - loss: 0.3884 - val_accuracy: 0.7500 - val_loss: 1.1402\n",
            "Epoch 47/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.8805 - loss: 0.3320 - val_accuracy: 0.6854 - val_loss: 1.1536\n",
            "Epoch 48/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9375 - loss: 0.1297 - val_accuracy: 0.7143 - val_loss: 1.0221\n",
            "Epoch 49/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.8879 - loss: 0.3229 - val_accuracy: 0.7417 - val_loss: 0.9652\n",
            "Epoch 50/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.8125 - loss: 0.4107 - val_accuracy: 0.6786 - val_loss: 1.0041\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 445ms/step - accuracy: 0.7155 - loss: 1.0033\n",
            "Test accuracy: 72.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"\")"
      ],
      "metadata": {
        "id": "vK97UW-rPUsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"GarbageMLModel\"  # Your repo name\n",
        "model_path = \"GarbageMLModel\""
      ],
      "metadata": {
        "id": "_9wWSSBmPjOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"GarbageMLModel_CS549.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA_hrwW9QuEi",
        "outputId": "a8c4c0cd-33e4-4e9f-b0d9-2d83067f01be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWJRuw4Fx--e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "repo_id = \"vincenthuynh/GarbageMLModel_CS549\"  # Replace with your username and desired repo name\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)"
      ],
      "metadata": {
        "id": "cEbE5XDfRYj1",
        "outputId": "be6d34b9-f82b-4b27-b98c-bf2260a69a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/vincenthuynh/GarbageMLModel_CS549', endpoint='https://huggingface.co', repo_type='model', repo_id='vincenthuynh/GarbageMLModel_CS549')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder, upload_file\n",
        "\n",
        "# Define repository information\n",
        "repo_id = \"vincenthuynh/GarbageMLModel_CS549\"  # Replace with your Hugging Face username and repo name\n",
        "\n",
        "# Upload the .h5 file\n",
        "upload_file(\n",
        "    path_or_fileobj=\"GarbageMLModel_CS549.h5\",  # Path to your saved model\n",
        "    path_in_repo=\"model.h5\",               # Desired path in the Hugging Face repo\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload trained model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "e402223c87e84b80b221ff4dbb39d973",
            "fe886e25c4dd4444ab1ce3d1dff7e6ae",
            "a047c62289c247579e7e609d89de0266",
            "c81f67f481c64befa2f91bbf32ec71ec",
            "cd44e9db705044c587bf3b32960b6372",
            "b5519d03251a4bd2baa0d5753248d1cc",
            "8cbb1b0713bd4258b5f794b6bd7fa9cb",
            "08aba7b9fd0f4c2ab19cdb37b51e4088",
            "f7b98a3b3c624eba8f01c9b89557653c",
            "54b5278192804eeea20d1310a170075f",
            "5462bb5709d347d68ac0606a87a43241"
          ]
        },
        "id": "zp6ABFm_Q9F9",
        "outputId": "d7ce7a51-9718-4f34-90f7-cf8493a40c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "GarbageMLModel_CS549.h5:   0%|          | 0.00/58.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e402223c87e84b80b221ff4dbb39d973"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/vincenthuynh/GarbageMLModel_CS549/commit/4dc1452963071021a8c5c277ec63615547a2dd4e', commit_message='Upload trained model', commit_description='', oid='4dc1452963071021a8c5c277ec63615547a2dd4e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vincenthuynh/GarbageMLModel_CS549', endpoint='https://huggingface.co', repo_type='model', repo_id='vincenthuynh/GarbageMLModel_CS549'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# 1. Training and Validation Accuracy/Loss Graphs\n",
        "def plot_training_history(history):\n",
        "    # Plot Training and Validation Accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Training and Validation Loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n",
        "\n",
        "# 2. Confusion Matrix\n",
        "def plot_confusion_matrix(model, generator):\n",
        "    # Predict the classes for the validation set\n",
        "    val_predictions = model.predict(generator)\n",
        "    predicted_classes = np.argmax(val_predictions, axis=1)\n",
        "    true_classes = generator.classes\n",
        "    class_labels = list(generator.class_indices.keys())\n",
        "\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('True Class')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(model, val_generator)\n",
        "\n",
        "# 3. Classification Report\n",
        "def print_classification_report(model, generator):\n",
        "    val_predictions = model.predict(generator)\n",
        "    predicted_classes = np.argmax(val_predictions, axis=1)\n",
        "    true_classes = generator.classes\n",
        "    class_labels = list(generator.class_indices.keys())\n",
        "\n",
        "    # Generate a classification report\n",
        "    report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "print_classification_report(model, val_generator)\n",
        "\n",
        "# 4. Sample Predictions\n",
        "def display_sample_predictions(generator, model, num_images=9):\n",
        "    class_labels = list(generator.class_indices.keys())\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(num_images):\n",
        "        # Get an image and label from the generator\n",
        "        img, label = generator.next()\n",
        "        prediction = model.predict(img)\n",
        "        predicted_class = np.argmax(prediction[0])\n",
        "        true_class = np.argmax(label[0])\n",
        "\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(img[0])\n",
        "        plt.title(f\"Pred: {class_labels[predicted_class]}\\nTrue: {class_labels[true_class]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_sample_predictions(val_generator, model)\n",
        "\n",
        "# 5. Model Architecture Visualization\n",
        "def visualize_model_architecture(model, save_path='model_architecture.png'):\n",
        "    plot_model(model, to_file=save_path, show_shapes=True, show_layer_names=True)\n",
        "    print(f\"Model architecture saved to {save_path}\")\n",
        "\n",
        "visualize_model_architecture(model)\n",
        "\n",
        "# 6. Data Augmentation Visualization\n",
        "def visualize_data_augmentation(generator, sample_image_path, num_images=6):\n",
        "    img = load_img(sample_image_path)  # Load the image\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        augmented_img = generator.random_transform(img_to_array(img))\n",
        "        plt.imshow(augmented_img / 255.0)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"Data Augmentation Examples\")\n",
        "    plt.show()\n",
        "\n",
        "# Provide a sample image path for visualization\n",
        "sample_image_path = list(train_generator.filepaths)[0]\n",
        "visualize_data_augmentation(train_datagen, sample_image_path)\n",
        "\n",
        "# 7. Accuracy per Class\n",
        "def plot_class_accuracy(model, generator):\n",
        "    val_predictions = model.predict(generator)\n",
        "    predicted_classes = np.argmax(val_predictions, axis=1)\n",
        "    true_classes = generator.classes\n",
        "    class_labels = list(generator.class_indices.keys())\n",
        "\n",
        "    # Generate the confusion matrix\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "    # Compute accuracy for each class\n",
        "    correct = np.diag(cm)\n",
        "    total = np.sum(cm, axis=1)\n",
        "    class_accuracy = correct / total\n",
        "\n",
        "    # Plot accuracy per class\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(class_labels, class_accuracy, color='skyblue')\n",
        "    plt.title(\"Accuracy per Class\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0, 1)  # Scale from 0 to 1\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_class_accuracy(model, val_generator)"
      ],
      "metadata": {
        "id": "xKP-66NRx_-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
